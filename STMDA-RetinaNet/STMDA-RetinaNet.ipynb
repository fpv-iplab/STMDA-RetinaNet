{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MDA-RetinaNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO/a9Yr96qb9DEJ4qx9Yr1M"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IeYPTFrX7BH_"},"source":["#STEP 1\n","run the first two blocks and restart the runtime\n","\n"]},{"cell_type":"code","metadata":{"id":"XxL9J_SB4n5L"},"source":["!!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install pyyaml==5.1 pycocotools>=2.0.1\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","!gcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PcKkfmBl4t_1"},"source":["!pip install detectron2==0.2.1 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8O7Om0l-7Mts"},"source":["#STEP2 \n","Replace at the following path ```../usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/``` the ```retinanet.py``` script with our ```retinanet.py```. <br>\n","Do the same for the ```fpn.py``` file at the path ```../usr/local/lib/python3.7/dist-packages/detectron2/modeling/backbone/```, ```evaltuator.py``` and ```coco_evaluation.py``` at ```../usr/local/lib/python3.7/dist-packages/detectron2/evaluation/```<br>\n","\n","Load the dataset in Google Drive and import it running the block below."]},{"cell_type":"code","metadata":{"id":"McBGQzl24zRA"},"source":["import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","import numpy as np\n","import cv2\n","import random\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","import logging\n","import os\n","from collections import OrderedDict\n","from torch.nn.parallel import DistributedDataParallel\n","import detectron2.utils.comm as comm\n","from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n","from detectron2.data import MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n","from detectron2.modeling import build_model\n","from detectron2.solver import build_lr_scheduler, build_optimizer\n","from detectron2.utils.events import CommonMetricPrinter, EventStorage, JSONWriter, TensorboardXWriter\n","import torch, torchvision\n","from detectron2.data.datasets import register_coco_instances,load_coco_json\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DnlOi8L988mS"},"source":["#STEP 3\n","Register you dataset using: <br>\n","```register_coco_instances(\"dataset_name_soruce_training\",{},\"path_annotations\",\"path_images\")```<br>\n","```register_coco_instances(\"dataset_name_target_training\",{},\"path_annotations\",\"path_images\")```<br>\n","```register_coco_instances(\"dataset_name_target2_training\",{},\"path_annotations\",\"path_images\")```<br>\n","```register_coco_instances(\"dataset_name_target_test\",{},\"path_annotations\",\"path_images\")```<br>\n","```register_coco_instances(\"dataset_name_target_test2\",{},\"path_annotations\",\"path_images\")```<br>\n","\n","these are the paths where will be saved the annotations produced at the end of the step 1<br>\n","```register_coco_instances(\"dataset_name_target_training\",{},\"path_annotations\",\"path_images\")```<br> \n","```register_coco_instances(\"dataset_name_target2_training\",{},\"path_annotations\",\"path_images\")```<br><br>\n","\n","import the folder utils in the same directory where is located this notebook file"]},{"cell_type":"markdown","metadata":{"id":"pkSaqf_Q9P7x"},"source":["#STEP 4\n","Run the latest block. <br>\n","With large dataset there are some problems with dataloaders. If you encounter these problems, restart the runtime and re-run all the boxes starting from step 2 (included) until it work. \n"]},{"cell_type":"code","metadata":{"id":"DOOfj8tw5QHS"},"source":["def do_train(cfg_source, cfg_target, cfg_target2,  model, resume = False, max_epochs_alpha=40, step_1 = True):\n","\n","    DatasetCatalog.clear()\n","    register_coco_instances(\"dataset_train_synthetic\", {}, \"Bellomo_Dataset_UDA/synthetic/Object_annotations/new_synthetic.json\", \"Bellomo_Dataset_UDA/synthetic/images\")\n","    register_coco_instances(\"init_dataset_train_real\", {}, \"Bellomo_Dataset_UDA/real_hololens/training/training_set.json\", \"Bellomo_Dataset_UDA/real_hololens/training\")\n","    register_coco_instances(\"init_dataset_train_real2\", {},\"Bellomo_Dataset_UDA/real_gopro/Training/training_set.json\",\"Bellomo_Dataset_UDA/real_gopro/Training\")\n","\n","    #dataset annotation created at the end of step 1\n","    register_coco_instances(\"dataset_train_real\", {}, \"output/new_train_holo.json\", \"Bellomo_Dataset_UDA/real_hololens/training\")\n","    register_coco_instances(\"dataset_train_real2\", {},\"output/new_train_gopro.json\",\"Bellomo_Dataset_UDA/real_gopro/Training\")\n","\n","    register_coco_instances(\"dataset_test_real\", {}, \"Bellomo_Dataset_UDA/real_hololens/test/test_set.json\", \"Bellomo_Dataset_UDA/real_hololens/test\")\n","    register_coco_instances(\"dataset_test_real2\", {},\"Bellomo_Dataset_UDA/real_gopro/Test/test_set.json\",\"Bellomo_Dataset_UDA/real_gopro/Test\")\n","\n","    logger = logging.getLogger(\"detectron2\")\n","\n","    model.train()\n","    print(model)\n","    optimizer = build_optimizer(cfg_source, model)\n","    scheduler = build_lr_scheduler(cfg_source, optimizer)\n","\n","    checkpointer = DetectionCheckpointer(\n","        model, cfg_source.OUTPUT_DIR, optimizer = optimizer, scheduler = scheduler\n","    )\n","    start_iter = (\n","        checkpointer.resume_or_load(cfg_source.MODEL.WEIGHTS, resume = resume).get(\"iteration\", -1) + 1\n","    )\n","    max_iter = cfg_source.SOLVER.MAX_ITER\n","\n","    periodic_checkpointer = PeriodicCheckpointer(\n","        checkpointer, cfg_source.SOLVER.CHECKPOINT_PERIOD, max_iter = max_iter\n","            )\n","\n","    writers = (\n","        [\n","            CommonMetricPrinter(max_iter),\n","            JSONWriter(os.path.join(cfg_source.OUTPUT_DIR, \"metrics.json\")),\n","            TensorboardXWriter(cfg_source.OUTPUT_DIR),\n","        ]\n","        if comm.is_main_process()\n","        else []\n","    )\n","    i = 1\n","    current_epoch = 0\n","    max_epoch = max_epochs_alpha #max iteration / min (data_len(source, target_1, target_2))\n","    data_len = 1502\n","\n","    data_loader_source = build_detection_train_loader(cfg_source)\n","    data_loader_target = build_detection_train_loader(cfg_target)\n","    data_loader_target2 = build_detection_train_loader(cfg_target2)\n","\n","    logger.info(\"Starting training from iteration {}\".format(start_iter))\n","\n","    with EventStorage(start_iter) as storage:\n","        for data_source, data_target, data_target2, iteration in zip(data_loader_source, data_loader_target, data_loader_target2, range(start_iter, max_iter)):\n","            iteration = iteration + 1\n","            storage.step()\n","\n","            if (iteration % data_len) == 0:\n","                current_epoch += 1\n","                i = 1\n","\n","            p = float( i + current_epoch * data_len) / max_epoch / data_len\n","            alpha = 2. / ( 1. + np.exp( -10 * p)) - 1\n","            i += 1\n","\n","            if alpha > 0.5:\n","                alpha = 0.5\n","\n","            loss_dict = model(data_source, \"source\", alpha, step_1)\n","            loss_dict_target = model(data_target, \"target_1\", alpha, step_1)\n","            loss_dict_target2 = model(data_target2, \"target_2\", alpha, step_1)\n","\n","            if step_1 == False:\n","                loss_dict[\"loss_cls\"] += loss_dict_target[\"loss_cls\"]\n","                loss_dict[\"loss_cls\"] += loss_dict_target2[\"loss_cls\"]\n","                loss_dict[\"loss_cls\"] *=0.33\n","\n","                loss_dict[\"loss_box_reg\"] += loss_dict_target[\"loss_box_reg\"]\n","                loss_dict[\"loss_box_reg\"] += loss_dict_target2[\"loss_box_reg\"]\n","                loss_dict[\"loss_box_reg\"] *=0.33\n","\n","            loss_dict[\"loss_r3\"] += loss_dict_target[\"loss_r3\"]\n","            loss_dict[\"loss_r3\"] += loss_dict_target2[\"loss_r3\"]\n","            loss_dict[\"loss_r3\"] *= 0.2\n","\n","            losses = sum(loss_dict.values())\n","            assert torch.isfinite(losses).all(), loss_dict\n","\n","            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n","            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n","            if comm.is_main_process():\n","                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n","\n","            optimizer.zero_grad()\n","            losses.backward()\n","            optimizer.step()\n","            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n","            scheduler.step()\n","\n","            if iteration - start_iter > 5 and (iteration % 20 == 0 or iteration == max_iter):\n","                for writer in writers:\n","                    writer.write()\n","            periodic_checkpointer.step(iteration)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tjO6XSa5qXpW"},"source":["from utils.annotator_fixer import fix_annotation\n","def setup_training(synthetic_dataset, target_1, target_2, wd=0.001, step=1, thr=0.8, max_epochs_alpha=40 ,iteration=60000, evaluate = False):\n","    cfg_source = get_cfg()\n","    cfg_source.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n","    cfg_source.DATASETS.TRAIN = (synthetic_dataset,)\n","    cfg_source.DATALOADER.NUM_WORKERS = 0\n","    cfg_source.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")\n","    cfg_source.SOLVER.IMS_PER_BATCH = 4\n","    cfg_source.SOLVER.BASE_LR = 0.0002\n","    cfg_source.SOLVER.MAX_ITER = iteration\n","    cfg_source.SOLVER.STEPS = (30000,)\n","    cfg_source.SOLVER.WEIGHT_DECAY = wd\n","    cfg_source.INPUT.MIN_SIZE_TRAIN = (0,)\n","    cfg_source.INPUT.MIN_SIZE_TEST = 0\n","    os.makedirs(cfg_source.OUTPUT_DIR, exist_ok=True)\n","    cfg_source.MODEL.RETINANET.NUM_CLASSES = 16\n","    model = build_model(cfg_source)\n","\n","    cfg_target = get_cfg()\n","    cfg_target.DATASETS.TRAIN = (target_1,)\n","    cfg_target.INPUT.MIN_SIZE_TRAIN = (0,)\n","    cfg_target.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n","    cfg_target.DATALOADER.NUM_WORKERS = 0\n","    cfg_target.SOLVER.IMS_PER_BATCH = 2\n","\n","    cfg_target2 = get_cfg()\n","    cfg_target2.DATASETS.TRAIN = (target_2,)\n","    cfg_target2.INPUT.MIN_SIZE_TRAIN = (0,)\n","    cfg_target2.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n","    cfg_target2.DATALOADER.NUM_WORKERS = 0\n","    cfg_target2.SOLVER.IMS_PER_BATCH = 2\n","\n","    if step == 1:\n","        step_1 = True\n","    else:\n","        step_1 = False\n","\n","    do_train(cfg_source, cfg_target, cfg_target2, model, max_epochs_alpha=max_epochs_alpha, step_1=step_1)\n","    model.score_threshold = thr\n","\n","    from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","    evaluator = COCOEvaluator(\"init_dataset_train_real\", cfg_source, False, output_dir=\"./output/\")\n","    val_loader = build_detection_test_loader(cfg_source, \"init_dataset_train_real\")\n","    inference_on_dataset(model, val_loader, evaluator, True)\n","\n","    len_holo = fix_annotation(\"Bellomo_Dataset_UDA/real_hololens/training/training_set.json\", \"./output/coco_instances_results.json\", \"./output/new_train_holo.json\")\n","\n","    from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","    evaluator = COCOEvaluator(\"init_dataset_train_real2\", cfg_source, False, output_dir=\"./output/\")\n","    val_loader = build_detection_test_loader(cfg_source, \"init_dataset_train_real2\")\n","    inference_on_dataset(model, val_loader, evaluator, True)\n","\n","    len_gopro = fix_annotation(\"Bellomo_Dataset_UDA/real_gopro/Training/training_set.json\", \"./output/coco_instances_results.json\", \"./output/new_train_gopro.json\")\n","\n","    if evaluate:\n","        #test Hololens\n","        from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","        evaluator = COCOEvaluator(\"dataset_test_real\", cfg_source, False, output_dir=\"./output/\")\n","        val_loader = build_detection_test_loader(cfg_source, \"dataset_test_real\")\n","        inference_on_dataset(model, val_loader, evaluator)\n","\n","        #test GoPro\n","        from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","        evaluator = COCOEvaluator(\"dataset_test_real2\", cfg_source, False, output_dir=\"./output/\")\n","        val_loader = build_detection_test_loader(cfg_source, \"dataset_test_real2\")\n","        inference_on_dataset(model, val_loader, evaluator)\n","\n","    return len_holo, len_gopro\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tK5SC_BBqZtk"},"source":["len_holo, len_gopro = setup_training(\"dataset_train_synthetic\", \"init_dataset_train_real\", \"init_dataset_train_real2\",step=1, max_epochs_alpha=40, thr=0.75, iteration=60000)\n","print(\"step 1 ended\")\n","len_holo, len_gopro = setup_training(\"dataset_train_synthetic\", \"dataset_train_real\", \"dataset_train_real2\", step=2, max_epochs_alpha=14, thr=0.8, iteration=20000)\n","print(\"step 2 ended\")\n","len_holo, len_gopro = setup_training(\"dataset_train_synthetic\", \"dataset_train_real\", \"dataset_train_real2\", step=2, max_epochs_alpha=14, thr=0.85, iteration=20000)\n","print(\"step 3 ended\")\n","len_holo, len_gopro = setup_training(\"dataset_train_synthetic\", \"dataset_train_real\", \"dataset_train_real2\", step=2, max_epochs_alpha=14, thr=0.9, iteration=20000, evaluate=True)\n","print(\"step 4 ended\")"],"execution_count":null,"outputs":[]}]}